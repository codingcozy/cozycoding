---
title: "물리 역학 시스템을 위한 강화 학습 대안적 접근"
description: ""
coverImage: "/assets/img/2024-08-26-ReinforcementLearningforPhysicalDynamicalSystemsAnAlternativeApproach_0.png"
date: 2024-08-26 18:03
ogImage: 
  url: /assets/img/2024-08-26-ReinforcementLearningforPhysicalDynamicalSystemsAnAlternativeApproach_0.png
tag: Tech
originalTitle: "Reinforcement Learning for Physical Dynamical Systems An Alternative Approach"
link: "https://medium.com/towards-data-science/rl-for-physical-dynamical-systems-an-alternative-approach-8e2269dc1e79"
isUpdated: false
---


## 유전 알고리즘(Genetic Algorithms) 소개 및 신경망과의 비교

![이미지](/assets/img/2024-08-26-ReinforcementLearningforPhysicalDynamicalSystemsAnAlternativeApproach_0.png)

# 물리 및 비선형 역학

고전적인, 견고한 및 최적의 방법을 통해 제어 이론은 현대 문명을 가능케 합니다. 정밀 가공, 통신, 현대 제조 등이 그것에 의존합니다. 제어 이론은 뉴턴의 법칙과 맥스웰의 방정식 등으로부터 파생된 물리학 방정식에서 제공하는 통찰력을 기반으로 발전해 왔습니다. 이러한 방정식은 물리 시스템에서 서로 다른 힘들의 상호 작용을 설명합니다. 이를 통해 우리는 방정식이 상태 간을 어떻게 이동하는지 이해하게 되는데, 여기서 상태란 "시스템을 충분히 설명하는 모든 정보의 집합"을 의미합니다 [1]. 이는 유체 역학의 압력이나 속도, 또는 전자 역학의 전하 및 전류 상태와 같은 변수로 표현될 수 있습니다. 시스템에 대한 방정식을 유도함으로써 우리는 시스템의 상태가 시간과 공간을 통해 어떻게 변하는지 예측할 수 있으며, 이 진화를 미적분 방정식의 형태로 표현할 수 있습니다. 이해를 바탕으로 특수하게 적용된 힘이 시스템의 원하는 상태나 출력값을 유지할 수 있도록 하는 제어를 적용할 수 있습니다. 일반적으로, 이 힘은 시스템의 출력을 바탕으로 계산됩니다. 예를 들어, 차량의 크루즈 컨트롤을 생각해보세요. 입력은 원하는 속도이고, 출력은 실제 속도입니다. 시스템은 엔진입니다. 상태 추정기는 속도를 관찰하고 출력과 입력 속도의 차이 및 제어 방법을 적용하는 방법을 결정합니다.

<div class="content-ad"></div>

그러나 이러한 성취들을 놓고 보면, 제어 이론은 상당한 제약사항에 직면하게 됩니다. 대부분의 제어 이론은 선형 시스템 또는 입력의 비례적 변화로 출력이 비례적으로 변하는 시스템을 중심으로 구축되어 있습니다. 이러한 시스템들은 상당히 복잡할 수 있지만, 우리는 이러한 시스템들에 대한 폭넓은 이해를 갖고 있으므로 심해 해저 잠수함부터 광산 장비, 우주선 등 모든 것을 실용적으로 제어할 수 있습니다.

그렇지만 스타니스와 울람이 언급한 것처럼, "비선형 과학이라는 용어를 사용하는 것은 동물학의 대부분을 코끼리가 아닌 동물 연구로 이야기하는 것과 같다." 지금까지 복잡한 물리 시스템을 제어하는 것에서 우리의 진전은 대부분 이러한 시스템을 선형 행동으로 한정시키는 방법을 찾아내어 이루어졌습니다. 이는 우리에게 효율성을 손해보게 만들 수 있습니다:

- 복잡한 시스템을 개별적으로 제어할 수 있는 구성 요소로 분해하여, 전체 시스템이 아닌 하위 시스템을 최적화하는 방식
- 더 단순하지만 효율적이지 않은 작동 모드에서 시스템을 운영하거나, 항공기 감속을 위한 활동적인 유동 제어 등 복잡한 물리학을 활용하지 않는 방식들

<div class="content-ad"></div>

- 예측할 수없는 또는 재해를 초래할 수 있는 엄격한 운영 조건 한계

고급 제조 기술, 향상된 공기역학 및 복잡한 통신 기술은 비선형 시스템 제어에 대해 더 나은 접근 방식을 적용할 수 있다.

비선형 동역학 시스템의 기본적인 특성은 입력에 대한 복잡한 반응이다. 비선형 시스템은 환경이나 상태의 작은 변화에도 극명하게 다양하게 변형된다. 유체 흐름을 지배하는 네비에-스톡스 방정식을 고려해보자: 같은 방정식 집합은 고요한 느린 흐르는 시냇물에서 거세게 흐르는 홍수까지 모두 기술하며, 홍수의 모든 회오리와 특징이 방정식 동역학 안에 포함된다.

비선형 시스템은 예측하기 어렵다는 어려움을 제시한다: 선형 시스템과는 달리 우리는 종종 시스템이 다음 상태로 전환될 때 어떻게 행동할지 쉽게 예측할 수 없다. 최선의 접근 방식은 일반적인 분석이나 광범위한 시뮬레이션을 통해 이루어지기 때문이다. 따라서 비선형 시스템에서는 두 가지 문제에 직면하게 된다: 시스템 식별 - 즉, 특정 상태에서 어떻게 행동할지 이해하는 것, 그리고 시스템 제어 - 주어진 입력에 대한 짧은 기간과 긴 기간 내 어떻게 변할지, 그리고 원하는 결과를 얻기 위해 어떤 입력을 하는지에 대한 문제이다.

<div class="content-ad"></div>

# 물리학을 위한 강화 학습

비선형 분석 및 제어가 계속해서 진전을 이루는 가운데, 전통적인 방정식 기반 방법을 사용하여 이러한 시스템을 활용하는 우리의 능력은 여전히 제한되어 있습니다. 그러나 컴퓨팅 파워와 센서 기술이 더 접근 가능해지면서 데이터 기반 접근 방식이 다른 접근 방식을 제공합니다.

데이터의 대량 증가로 인해 기계 학습(ML) 접근 방식이 등장했으며, 강화 학습(RL)은 비선형 동적 시스템을 더 효과적으로 제어하는 과제에 대한 새로운 해결책을 제공합니다. 이미 운전 자동차부터 전략 및 컴퓨터 게임에 이르기까지 다양한 환경에서 성공을 거듭하고 있는 강화 학습은 알고리즘 또는 에이전트를 훈련하여 "불확실성 하에서 결정을 내리고 장기적 이익을 극대화하기 위해 시행착오를 통해 학습하는 방법" [1]입니다. 간단히 말해, 강화 학습 알고리즘은 시스템 식별 및 제어 최적화 문제에 접근하며, 주어진 입력 조치가 원하는 결과로 이끄는 지 예측 개발을 위해 환경 샘플링을 통해 이를 수행합니다. RL 알고리즘 또는 에이전트는 시스템 상태를 기반으로 행동 정책을 적용하고 시스템에 대한 더 많은 정보를 분석함에 따라 이 정책을 개선합니다.

많은 강화 학습 알고리즘은 신경망을 사용하여 상태를 최적의 행동으로 매핑하는 함수를 개발하는 데 기초합니다. RL 문제는 상태-행동-보상 튜플로 구성할 수 있습니다. 주어진 상태에서 특정 조치가 특정 보상으로 이끄는 경우를 의미합니다. 신경망은 시스템 전체에서 상태-행동-보상 튜플 함수를 정확하게 근사화할 수 있는 범용 함수 근사자 역할을 합니다. 이렇게 하려면 시스템이나 환경을 탐구하여 새로운 지식을 습득한 후 추가 데이터를 활용하여 정책을 개선해야 합니다. 강화 학습 알고리즘은 수학을 어떻게 활용하여 탐색, 활용, 두 가지 사이의 균형을 유지하는지에 따라 구분됩니다.

<div class="content-ad"></div>

그러나 신경망은 몇 가지 도전 과제를 안겨줍니다:

- 자원 요구 사항. 각 상태에 대한 보상을 결정하고 취해야 할 최상의 조치를 결정할 수 있는 함수를 추정하기 위해 신경망을 사용하는 데 상당한 시간과 데이터가 필요할 수 있습니다.

- 설명 가능성. 신경망이 해결책에 도달하는 방식을 이해하기가 어려운 경우가 많은데, 이는 실질적인 통찰력을 제공하기 어렵게 하며 신경망의 행동을 예측하거나 제한하는 것을 어렵게 할 수 있습니다. 설명 가능성은 특히 물리적 시스템에서 중요한데, 수세기에 걸쳐 발전한 강력한 수학적 도구를 사용하여 시스템에 대한 추가 통찰력을 얻을 수 있기 때문입니다.

이러한 도전 과제를 해결하기 위한 전이 학습 및 위상 분석과 같은 접근 방법이 있지만, 이러한 도전 과제들은 강화 학습의 더 광범위한 적용을 방해하는 장애물로 남아 있습니다. 그러나 우리가 특히 물리적 시스템을 고려하는 경우에는 대안적 접근 방법이 유용할 수 있습니다. 우리가 논의하고 있는 물리적 시스템은 수학적 방정식에 의해 정의되거나 매우 잘 설명될 수 있습니다. 완전히 임의의 함수를 개발하는 대신, 일반적인 수학 연산자(산술, 대수, 초월 함수(sine, e^x 등))로 이루어진 식을 찾으려고 할 수 있습니다. 이를 위한 수단으로 유전 알고리즘을 사용할 것입니다. [2]에서 설명된 바와 같이, 유전 알고리즘은 함수의 공간을 탐색하기 위해 함수들을 무작위로 생성하고 유망한 후보들의 돌연변이와 교배를 통해 솔루션을 활용하고 개선시킬 수 있습니다.

<div class="content-ad"></div>

그래서 신경망은 대부분의 강화학습 문제에서 우승자이지만, 물리 역학에서는 새로운 도전자가 나타납니다. 이제 일반 알고리즘 접근 방식을 자세히 살펴보고 주요 강화학습 알고리즘인 Soft Actor Critic과 어떻게 대비되는지 살펴볼 것입니다. 우리는 AWW Sagemaker 실험을 통해 물리학 기반 체육관에서 둘 다를 평가할 것입니다. 결과를 평가하고 결론을 논의하며 추후 조치를 제안할 것입니다.

강화학습이 직면하는 두 가지 문제를 상기해보면, 환경을 탐사하고 발견된 정보를 활용하는 것입니다. 탐색은 어떤 상태에 있을 가능성을 고려해 최상의 정책을 찾기 위해 필요합니다. 탐색을 제대로 수행하지 않으면 전역 최적해를 놓칠 수 있고, 알고리즘은 충분히 모든 상태에서 성공하기 위한 범용화를 잘 수행하지 못할 수 있습니다. 활용은 현재의 솔루션을 최적화하기 위해 필요합니다. 그러나 알고리즘은 특정 솔루션을 정제할 때 시스템을 더 탐색할 수 있는 능력을 상실합니다.

Soft Actor Critic (SAC)는 강력한 Actor-Critic 강화학습 방법의 정제판입니다. Actor-Critic 계열의 알고리즘은 상태 값 및 관련 보상의 추정치를 최적화하는 입력 정책을 분리함으로써 탐색/활용 균형을 접근합니다. 알고리즘이 새로운 정보를 수집하는 동안 각 성능 추정기를 업데이트합니다. Actor-Critic에는 구현에 대한 다양한 미묘한 점이 있으므로, 관심 있는 독자는 책이나 온라인 자습서를 참고해야 합니다. SAC는 비평가를 최적화하여 비평가가 예상한 것과 크게 다른 보상을 가진 상태를 탐색하는 것을 선호합니다. OpenAI에서 SAC에 대한 자세한 설명을 제공합니다.

이 실험에서는 SAC의 Coax 구현을 사용합니다. Coach와 Spinning Up을 포함한 여러 강화학습 라이브러리를 살펴봤지만, Coax는 현재 Python 빌드와 대부분 호환될 것으로 보여 제일 잘 작동하는 것 중 하나였습니다. Coax 라이브러리에는 PPO, TD3, DDPG 등 다양한 강화학습 알고리즘들이 포함되어 있으며, gymnasium과 잘 작동합니다.

<div class="content-ad"></div>

Actor-critic 방법(SAC와 같은)은 일반적으로 함수 근사기로서 신경망을 통해 구현됩니다. 지난 시간에 우리가 논의했던 대로, 시스템을 탐색하고 잠재적인 제어 정책을 활용하는 또 다른 가능한 접근 방법이 있습니다. 유전 알고리즘은 가능한 해결책을 무작위로 생성하여 탐색하고 유망한 정책을 이용하기 위해 다양한 해결책의 요소들을 변이하거나 결합하는 방식으로 활용합니다. 이 경우 함수 근사화의 대체 수단으로 유전 알고리즘의 유전 프로그래밍 변형을 평가할 것이며, 구체적으로는 상수, 상태 변수 및 수학 함수를 포함하는 함수 트리를 잠재적인 컨트롤러로 사용할 것입니다.

주어진 유전 프로그래밍(GP) 알고리즘은 [2]에서 채택된 것과 유사하지만, 해당 텍스트가 사용한 tournament 대신에 이 구현은 각 세대의 상위 64% (33% 아래의 Nn)를 변이의 대상으로 선정하고 나머지 부분은 다음 해결책 공간의 탐색을 위해 재생성합니다. 각 세대의 개별 트리를 생성하기 위해, 성장 함수는 임의로 산술 함수(+, -, *, /) 및 초월 함수(e^x, cos(x) 등)를 호출하여 상수 또는 상태 변수를 Leaf로 사용하여 가지를 생성합니다. 폴란드식 표기법을 기반으로 한 식을 구성하기 위해 반복 호출을 사용하며 ([2]에서는 LISP를 통해 구현되었고, 저는 Python으로 적응시켰습니다) 0으로 나누는 경우 및 수학적 일관성을 보장하기 위한 규칙이 있어 각 가지가 올바르게 상수 또는 센서 값으로 끝나도록 합니다. 개념적으로, 방정식 트리는 다음과 같이 나타낼 수 있습니다:

![equation tree](/assets/img/2024-08-26-ReinforcementLearningforPhysicalDynamicalSystemsAnAlternativeApproach_1.png)

이는 컨트롤러 b = sin(s1) + e^(s1*s2/3.23) - 0.12으로, 스크립트에 의해 다음과 같이 작성됩니다: — + sin s1 e^ / * s1 s2 3.23 0.12, 여기서 s는 상태 변수를 나타냅니다. 처음에는 혼란스러울 수 있지만 몇 가지 예시를 적어보면 이해가 더 쉬워집니다.

<div class="content-ad"></div>

나무 세대가 모두 생성되면 각각의 나무는 성능을 평가하기 위해 환경을 통과합니다. 그런 다음 나무들은 달성된 보상에 기반하여 제어 성능에 대해 순위가 매겨집니다. 원하는 성능이 달성되지 않으면, 성능이 가장 우수한 나무는 보존되고 상위 66%는 교차(두 나무 요소를 교환), 잘라내고 증가(나무 요소를 교체), 축소(상수로 나무 요소를 교체) 또는 다시 매개변수화(나무의 모든 상수를 교체)에 의해 변이됩니다. 이것은 가장 유망한 솔루션들을 탐색할 수 있게 합니다. 솔루션 공간을 계속적으로 탐색하기 위해 성능이 낮은 솔루션들은 무작위 새로운 나무로 대체됩니다. 각 연속 세대는 그 후 가장 우수한 솔루션들의 복제 또는 변이와 무작위 새 개체들의 혼합입니다.

나무들은 환경 내에서 무작위 시작 위치에 대해 테스트됩니다. 결과를 왜곡하는 "운이 좋은" 시작 상태를 방지하기 위해 (모델의 과적합에 비유됨), 나무들은 다른 여러 무작위 시작 상태 집합에 대해 테스트됩니다.

유전 프로그래밍의 하이퍼파라미터에는 다음이 포함됩니다:

![hyperparameters](/assets/img/2024-08-26-ReinforcementLearningforPhysicalDynamicalSystemsAnAlternativeApproach_2.png)

<div class="content-ad"></div>

깃허브에서 주석된 코드를 확인할 수 있습니다. 제가 취미로 코딩을 하고 있기 때문에 코드가 다소 어렵지만 이해하기 쉽도록 했습니다. 파이썬적이지 않거나 일반적으로 나쁜 코딩 습관이 있을 수 있습니다.

## 접근 방식 평가하기

두 알고리즘은 두 가지 다른 체육관 환경에서 평가되었습니다. 첫 번째는 체육관 재단에서 제공하는 단순한 진자 환경입니다. 역진자는 간단한 비선형 동역학 문제입니다. 행동 공간은 진자에 적용할 수있는 연속 토크입니다. 관찰 공간은 상태와 동일하며 x, y 좌표 및 각속도입니다. 목표는 진자를 세울 수 있는 것입니다. 두 번째는 동일한 체육관이지만 관찰에 무작위 잡음이 추가되었습니다. 잡음은 평균이 0이고 분산이 0.1인 정규 분포로 현실적인 센서 측정을 모방합니다.

RL 개발의 가장 중요한 부분 중 하나는 적절한 보상 함수를 설계하는 것입니다. 주어진 RL 문제를 해결할 수있는 많은 알고리즘이 있지만, 이러한 알고리즘이 최적화하기 위한 적절한 보상을 정의하는 것은 특정 문제에 대해 특정 알고리즘을 성공적으로 만들기 위한 핵심 단계입니다. 결과를 비교할 수 있도록 우리의 보상은 각 궤적에 대해 누적 보상과 평균 보상을 추적합니다. 이를 쉽게 하기 위해 각 환경을 일정한 시간 단계로 실행하고 각 시간 단계마다 에이전트가 목표 상태로부터 얼마나 멀리 떨어져 있는지에 따라 음의 보상을 받습니다. Pendulum gym은 이를 기본적으로 처리하며, 200단계에서 자르고 진자가 수직에서 얼마나 멀리 떨어져 있는지에 따라 음의 보상을 제공하며, 각 시간 단계마다 0에서 최대 보상을 부여합니다. 두 접근 방식을 비교하기 위해 평균 보상을 사용할 것입니다.

<div class="content-ad"></div>

저희 목표는 각각의 RL 프레임워크 수렴 속도를 평가하는 것입니다. 이를 위해 AWS Sagemaker Experiments를 사용하여 수행할 것인데, 이를 통해 반복 또는 CPU 시간별로 메트릭(현재 보상과 같은)과 매개변수(활성 하이퍼파라미터와 같은)을 자동으로 추적할 수 있습니다. 이런 모니터링은 파이썬 도구를 통해 수행할 수 있지만, Experiments는 실행 매개변수와 성능을 간편하게 추적 및 색인화하고 컴퓨팅 자원을 복제하는 기능을 제공합니다. 실험을 설정하기 위해 AWS에서 제공한 예제를 적용했습니다. SAC 및 GP 알고리즘은 먼저 로컬 Jupyter 노트북에서 확인한 후 git 저장소에 업로드했습니다. 각 알고리즘은 자체 저장소와 Sagemaker 노트북을 가지고 있습니다. 실행 매개변수는 실행을 분류하고 서로 다른 실험 설정의 성능을 추적하기 위해 저장됩니다. 우리는 두 알고리즘을 비교하기 위해 측정하고 싶은 종속 변수인 실행 메트릭(보상 및 상태 벡터)을 측정하고자 합니다. Experiments는 CPU 시간과 반복을 자동으로 기록합니다.

이러한 실험을 통해, 잘 개발된 성숙한 RL 알고리즘인 SAC과 같은 챔피언의 성능을 이 외교하지 않은 아마추어 코더가 코딩한 경쟁자와 비교할 수 있습니다. 이 실험을 통해 복잡하고 비선형 시스템을 위한 컨트롤러를 개발하는 다양한 접근 방법에 대한 통찰을 얻을 수 있습니다. 다음 파트에서는 결과를 검토하고 토론하며 추가 조치에 대해 논의할 것입니다.

첫 번째 실험은 기본 펜듈럼 체육관이었는데, 이때 알고리즘은 펜듈럼을 세로로 유지하기 위해 적용해야 할 올바른 토크를 결정하려고 합니다. 일정 시간이 지나면 종료되며, 펜듈럼이 세로에서 얼마나 멀리 떨어져 있는지에 따라 음수의 보상을 제공합니다. Sagemaker 실험에서 실행되기 전에 SAC 및 GP 알고리즘은 로컬 머신에서 실행하여 수렴 여부를 확인했습니다. 실험에서 실행하는 것은 비교 가능한 컴퓨팅 시간을 더 잘 추적할 수 있었습니다. 컴퓨팅 시간 대 반복당 평균 보상의 결과는 아래와 같습니다:

![image](/assets/img/2024-08-26-ReinforcementLearningforPhysicalDynamicalSystemsAnAlternativeApproach_3.png)

<div class="content-ad"></div>


<img src="/assets/img/2024-08-26-ReinforcementLearningforPhysicalDynamicalSystemsAnAlternativeApproach_4.png" />

GP가 덜 성숙한 알고리즘임에도 불구하고, SAC보다 훨씬 적은 계산 요구량으로 해결책에 도달했습니다. Local 실행으로 완료하려면 SAC가 수렴하는 데 약 40만번의 반복이 필요했고, 여러 시간이 소요되었습니다. Local 인스턴스는 훈련 중 SAC 진행 상황을 기록하는 방식으로 프로그래밍되었습니다. 재밌는 점은 SAC가 진폭 상단으로 진폭을 이동하는 법을 배우는 것에서 진폭을 유지하는 방법을 배우고 그 둘을 결합하는 방식으로 진행된다는 것입니다. 이것은 SAC가 진폭을 안정하게 유지하는 법을 배우는 시기로 보입니다. GP의 경우 보상이 단계적으로 증가하는 것을 볼 수 있습니다. 이는 가장 성능이 우수한 함수 트리를 항상 보존하여, 더 나은 컨트롤러가 계산될 때까지 최상의 보상이 안정적으로 유지되기 때문입니다.

두 번째 실험은 상태 측정에 가우시안 노이즈(0, 0.1)를 추가한 것입니다. 노이즈 없는 상황과 유사한 결과를 보여주나 수렴 시간이 길어졌습니다. 아래 결과를 확인할 수 있습니다. 다시 한번, GP가 SAC를 능가했습니다.

<img src="/assets/img/2024-08-26-ReinforcementLearningforPhysicalDynamicalSystemsAnAlternativeApproach_5.png" />


<div class="content-ad"></div>


![image](/assets/img/2024-08-26-ReinforcementLearningforPhysicalDynamicalSystemsAnAlternativeApproach_6.png)

두 경우 모두 GP가 이전 예제와 마찬가지로 SAC보다 빠르게 수행됨을 확인할 수 있습니다 (로컬 수렴하지만 AWS에 계산 시간을 지불하고 싶지 않았습니다!). 그러나 여러분 중 많은 분들이 이미 알고 있겠지만, 이는 기계 학습과 물리 시스템 측면에서 매우 기본적인 비교입니다. 예를 들어, 하이퍼파라미터 튜닝은 다른 결과를 초래할 수 있습니다. 그럼에도 불구하고, 이는 경쟁 알고리즘에 대한 유망한 시작이며, 더 깊이 조사할 가치가 있다는 것을 나타냅니다.

장기적으로 볼 때, GP는 SAC와 같은 신경망 기반 접근 방식보다 여러 가지 이점을 제공할 수 있다고 생각합니다:

- 설명 가능성. GP가 찾는 방정식이 복잡할 수는 있지만 투명합니다. 전문가는 방정식을 간소화하여 결정된 솔루션의 물리학적 통찰력을 제공하는 데 도움을 줄 수 있습니다. 적용 가능한 영역을 결정하고 제어에 대한 신뢰도를 높이는 데 유용합니다. 설명 가능성은 계속 연구 중인 분야이지만, 신경망에 대한 도전이 남아 있습니다.


<div class="content-ad"></div>

- 정보 제공: ML. GP는 분석 중인 시스템에 대한 통찰력을 더 쉽게 적용할 수 있게 합니다. 예를 들어, 시스템이 사인 함수적인 행동을 보인다는 것이 알려져 있다면, GP 알고리즘은 더 많은 사인 함수적인 해결책을 시도할 수 있도록 적응될 수 있습니다. 반면, 유사하거나 단순화된 시스템에 대해 이미 알려진 해결책이 있다면, 해당 해결책을 알고리즘에 미리 주입할 수 있습니다.

- 안정성: 간단한 안전장치의 추가로 수학적 유효성 및 절대값 제한을 유지하여, GP 접근 방식은 안정적으로 유지됩니다. 각 세대마다 최고 수행자가 유지된다면 솔루션이 수렴될 것이지만, 수렴 시간의 한계는 보장되지 않습니다. 일반적인 RL의 신경망 접근법은 이러한 보장을 갖지 않습니다.

- 발전 기회: GP는 비교적 미숙합니다. 여기서 구현한 SAC는 적용 가능한 여러 방법 중 하나였으며, 신경망은 성능 향상을 위해 많은 노력을 기울인 케이스입니다. GP는 이와 같은 최적화의 혜택을 받지 못했으며, 제 구현은 효율성보다는 기능을 중심으로 구축되었습니다. 그럼에도 불구하고 SAC에 대해 잘 수행되었고, 보다 전문적인 개발자들로부터의 추가 향상은 효율성 측면에서 큰 이익을 제공할 수 있습니다.

- 병렬화와 모듈화: 개별 GP 방정식은 NNs에 비해 간단하기 때문에 계산 비용은 환경을 반복적으로 실행하는 것에 기인하며, NNs의 환경 실행 및 역전파와 달리 비교적 간단합니다. 다른 프로세서에 걸쳐 "다양한 GP 방정식 트리의 숲"을 나누어 분산하여 컴퓨팅 속도를 획기적으로 향상시키는 것이 쉬울 것입니다.

<div class="content-ad"></div>

그러나 신경망 접근 방식이 보다 널리 사용되는 이유는 다음과 같습니다:

- 범위. 신경망은 범용 함수 근사기입니다. GP는 함수 트리에서 정의된 용어에 한정됩니다. 따라서 신경망 기반 접근 방식은 훨씬 더 다양하고 복잡한 상황을 다룰 수 있습니다. 나는 GP를 사용해서 스타크래프트를 하거나 자동차를 운전하려고 하지 않겠어요.

- 추적. GP는 무작위 탐색의 정제된 버전으로, 실험에서 볼 수 있듯이 성능 향상이 중단됩니다.

- 성숙도. 다양한 신경 기반 알고리즘에서 수행된 포괄적인 작업으로, 문제에 빠르게 적용하기 위해 계산 효율성을 최적화한 기존 방법을 쉽게 찾을 수 있습니다.

<div class="content-ad"></div>

머신 러닝 관점에서 볼 때, 우리는 이러한 알고리즘을 활용할 수 있는 가능성을 아직 제대로 탐색하지 못했습니다. 고려해야 할 몇 가지 후속 조치는 다음과 같습니다:

- 하이퍼파라미터 튜닝

- 컨트롤러의 간소화, 예를 들어 GP의 제어 입력 항목 수에 대한 보상을 처벌화하는 것

- 컨트롤러의 효율성, 예를 들어 보상에서 제어 입력의 크기를 제거하는 것

<div class="content-ad"></div>

위에서 설명한 대로 GP 모니터링 및 알고리즘 개선을위한 테이블 태그를 변경하십시오.

물리학적인 관점에서 이 실험은 보다 현실적인 시나리오로 나아가는 출발점 역할을 합니다. 더 복잡한 시나리오에서는 NN 접근 방식이 GP를 따라잡거나 능가할 것으로 예상됩니다. 가능한 추후 작업에는 다음이 포함됩니다:

- VanderPol 방정식 또는 고차원과 같은 더 복잡한 동역학.

- 완전한 상태 관측 가능성 대신 제한된 관측 가능성.

<div class="content-ad"></div>

- 편미분 방정식 시스템 및 최적화 컨트롤러 위치 및 입력

[1] E. Bilgin, Mastering Reinforcement Learning with Python: Build next-generation, self-learning models using reinforcement learning techniques and best practices (2020), Packit Publishing

[2] T Duriez, S. Brunton, B. Noack, Machine Learning Control- Taming Nonlinear Dynamics and Turbulence (2017), Spring International Publishing